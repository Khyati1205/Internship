{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d66918e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\country head\\anaconda3\\lib\\site-packages (4.8.3)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: idna in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\country head\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\country head\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14e0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a80dd7",
   "metadata": {},
   "source": [
    "Question 1) Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 \n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the \n",
    "location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8df76e96",
   "metadata": {},
   "source": [
    "# Now we will download the webdriver for the Web Browser.Steps for download are'\n",
    "-Check the version of your brower. \n",
    "-go to the link: \"https//chromedriver.chromium.org/downloads\"\n",
    "- Check the version of your version of your browser  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f51c47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect to the driver : \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2d3b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the naukri web page on the automated web browser\n",
    "driver.get(\" https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc70952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering the designation and location based on the question: \n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5cc655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4575e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "search= driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a92fb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_tittle = []\n",
    "Job_location = []\n",
    "company_name = []\n",
    "expereince_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf93a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarpping Job tittle from the above page \n",
    "tittle_tags=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in tittle_tags[0:10]:\n",
    "    tittle=i.text\n",
    "    Job_tittle.append(tittle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b59e8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the location from the page \n",
    "location_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for l in location_tags[0:10]:\n",
    "    location=l.text\n",
    "    Job_location.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9570acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the names of the company from the page\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for c in company_tags[0:10]:\n",
    "    company=c.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0294a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the expereince\n",
    "expereince_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for e in expereince_tags[0:10]:\n",
    "    expereince = e.text\n",
    "    expereince_required.append(expereince)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd82cab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Job_tittle), len(Job_location), len(company_name), len(expereince_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90efbf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>Company</th>\n",
       "      <th>expereince</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cynosure Corporate Solutions</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Environmental Sustainability Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>PerkinElmer, Inc.</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst III</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compliance Rule Writer/Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Paypal</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>NNE Pharmaplan</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Ingersoll Rand</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst - EdTech</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Talentstack</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Unusual Hire</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Engineer/Data Analyst</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - Contractual</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Search Advisers Services Guj</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0                               Data Analyst   \n",
       "1  Environmental Sustainability Data Analyst   \n",
       "2                           Data Analyst III   \n",
       "3        Compliance Rule Writer/Data Analyst   \n",
       "4                               Data Analyst   \n",
       "5                               Data Analyst   \n",
       "6                      Data Analyst - EdTech   \n",
       "7                               Data Analyst   \n",
       "8                 Data Engineer/Data Analyst   \n",
       "9                 Data Analyst - Contractual   \n",
       "\n",
       "                                            location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8  Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                        Company expereince  \n",
       "0  Cynosure Corporate Solutions    2-7 Yrs  \n",
       "1             PerkinElmer, Inc.    2-6 Yrs  \n",
       "2                       Walmart    3-7 Yrs  \n",
       "3                        Paypal    5-8 Yrs  \n",
       "4                NNE Pharmaplan    1-2 Yrs  \n",
       "5                Ingersoll Rand    3-6 Yrs  \n",
       "6                   Talentstack    2-6 Yrs  \n",
       "7                  Unusual Hire    1-4 Yrs  \n",
       "8                 Tech Mahindra   6-11 Yrs  \n",
       "9  Search Advisers Services Guj    2-3 Yrs  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'title':Job_tittle,'location': Job_location,'Company':company_name,'expereince':expereince_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa45251",
   "metadata": {},
   "source": [
    "Q2:Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the \n",
    "location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results youget.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e939f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect to the driver : \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dea9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\" https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f8f832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13a31030",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fc6bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "search= driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c7abe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_tittle = []\n",
    "Job_location = []\n",
    "company_name = []\n",
    "expereince_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4946acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarpping Job tittle from the above page \n",
    "tittle_tags=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in tittle_tags[0:10]:\n",
    "    tittle=i.text\n",
    "    Job_tittle.append(tittle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06477e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the location from the page \n",
    "location_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for l in location_tags[0:10]:\n",
    "    location=l.text\n",
    "    Job_location.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51d35380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the names of the company from the page\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for c in company_tags[0:10]:\n",
    "    company=c.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b9db7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the expereince\n",
    "expereince_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for e in expereince_tags[0:10]:\n",
    "    expereince = e.text\n",
    "    expereince_required.append(expereince)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05b65d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Job_tittle), len(Job_location), len(company_name), len(expereince_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58c24f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>Company</th>\n",
       "      <th>expereince</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Professional - IBM SPSS Statistic...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Mumbai, Pune, Chennai</td>\n",
       "      <td>Hexaware Technologies</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior data scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist_NLP</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>5-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Machine Learning (AI) Architect</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Persistent</td>\n",
       "      <td>5-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sr. Data scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "      <td>PayU</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Manager - Innovations Hub - Machine Learning</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>PwC</td>\n",
       "      <td>7-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>8-12 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Data Science Professional - IBM SPSS Statistic...   \n",
       "1                            Data Science Specialist   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3                              Senior data scientist   \n",
       "4                                 Data Scientist_NLP   \n",
       "5                                     Data Scientist   \n",
       "6                    Machine Learning (AI) Architect   \n",
       "7                                 Sr. Data scientist   \n",
       "8       Manager - Innovations Hub - Machine Learning   \n",
       "9                                Lead Data Scientist   \n",
       "\n",
       "                                            location                Company  \\\n",
       "0  Bangalore/Bengaluru, Noida, Mumbai, Pune, Chennai  Hexaware Technologies   \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...              Accenture   \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...              Accenture   \n",
       "3  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics   \n",
       "4  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics   \n",
       "5  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics   \n",
       "6  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...             Persistent   \n",
       "7   Hybrid - Bangalore/Bengaluru, Mumbai (All Areas)                   PayU   \n",
       "8  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...                    PwC   \n",
       "9                                Bangalore/Bengaluru                Genpact   \n",
       "\n",
       "  expereince  \n",
       "0    5-8 Yrs  \n",
       "1    2-4 Yrs  \n",
       "2    6-8 Yrs  \n",
       "3    4-8 Yrs  \n",
       "4   5-11 Yrs  \n",
       "5    3-7 Yrs  \n",
       "6   5-12 Yrs  \n",
       "7    1-6 Yrs  \n",
       "8    7-9 Yrs  \n",
       "9   8-12 Yrs  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'title':Job_tittle,'location': Job_location,'Company':company_name,'expereince':expereince_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5915d1",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get thewebpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results youget.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "725ad064",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver : \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")\n",
    "\n",
    "driver.get(\" https://www.naukri.com/\")\n",
    "\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "search= driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "salary_filter = driver.find_element(By.ClASS_NAME,'//span[@title=\"3-6 Lakhs\"]')\n",
    "salary_filter.click()\n",
    "\n",
    "Location.filter = driver.find_elements(By.XPATH,'//label[@for=\"chk-3-6 Lakhs-ctcFilter-\"]')\n",
    "Location.filter.click()\n",
    "\n",
    "Job_tittle = []\n",
    "Job_location = []\n",
    "company_name = []\n",
    "expereince_required = []\n",
    "\n",
    "# scarpping Job tittle from the above page\n",
    "for i in tittle_tags[0:10]:\n",
    "    tittle=i.text\n",
    "    Job_tittle.append(tittle)\n",
    "\n",
    "#scrapping the location from the page \n",
    "location_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for l in location_tags[0:10]:\n",
    "    location=l.text\n",
    "    Job_location.append(location)\n",
    "    \n",
    "# scrapping the names of the company from the page\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for c in company_tags[0:10]:\n",
    "    company=c.text\n",
    "    company_name.append(company)\n",
    "\n",
    "# scrapping the expereince\n",
    "expereince_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for e in expereince_tags[0:10]:\n",
    "    expereince = e.text\n",
    "    expereince_required.append(expereince)\n",
    "    \n",
    "#Creating the dataFrame\n",
    "df=pd.DataFrame({'title':Job_tittle,'location': Job_location,'Company':company_name,'expereince':expereince_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5baff",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e675cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect to the driver : \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d1e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_select = driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "Product_select.send_keys('Sunglasses')\n",
    "\n",
    "search= driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "\n",
    "Brand_Name = []\n",
    "Product_Description = []\n",
    "Product_Price = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1760ef07",
   "metadata": {},
   "source": [
    "Product_select = driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "Product_select.send_keys('Sunglasses')\n",
    "\n",
    "search= driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "\n",
    "Brand_Name = []\n",
    "Product_Description = []\n",
    "Product_Price = []\n",
    "\n",
    "Brand_tags = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for b in Brand_tags[0:40]:\n",
    "    brand = b.text\n",
    "    Brand_Name.append(brand)\n",
    "    \n",
    "Description_tags = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for d in Description_tags[0:40]:\n",
    "    Product = d.text\n",
    "    Product_Description.append(Product)   \n",
    "\n",
    "Price_tags = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for p in Price_tags[0:40]:\n",
    "    Price = p.text\n",
    "    Product_Price.append(Price)\n",
    "    \n",
    "df= pd.DataFrame({'brand':Brand_Name,'description':Product_Description,'price':Product_Price})\n",
    "df\n",
    "\n",
    "# changing to switch the page\n",
    "Next= driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "Next.click()\n",
    "\n",
    "Brand_tags = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for b in Brand_tags[41:80]:\n",
    "    brand = b.text\n",
    "    Brand_Name.append(brand)\n",
    "    \n",
    "    Description_tags = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for d in Description_tags[41:80]:\n",
    "        Product = d.text\n",
    "        Product_Description.append(Product)   \n",
    "\n",
    "Price_tags = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for p in Price_tags[41:80]:\n",
    "    Price = p.text\n",
    "    Product_Price.append(Price)\n",
    "\n",
    "df= pd.DataFrame({'brand':Brand_Name,'description':Product_Description,'price':Product_Price})\n",
    "df\n",
    "\n",
    "Next_again= driver.find_element(By.XPATH,\"//a[@class='_1LKTO3'][last()]\")\n",
    "Next_again.click()  \n",
    "\n",
    "Brand_tags = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for b in Brand_tags[81:100]:\n",
    "    brand = b.text\n",
    "    Brand_Name.append(brand)\n",
    "    \n",
    "    Description_tags = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for d in Description_tags[81:100]:\n",
    "        Product = d.text\n",
    "        Product_Description.append(Product)   \n",
    "\n",
    "Price_tags = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for p in Price_tags[81:100]:\n",
    "    Price = p.text\n",
    "    Product_Price.append(Price)\n",
    "\n",
    "df= pd.DataFrame({'brand':Brand_Name,'description':Product_Description,'price':Product_Price})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd848b54",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the \n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2713670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect to the driver : \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "045999e6",
   "metadata": {},
   "source": [
    "Product_select = driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "Product_select.send_keys('Sneakers')\n",
    "\n",
    "search= driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "\n",
    "start = 0\n",
    "end = 2 \n",
    "\n",
    "\n",
    "for page in range(start,end):\n",
    "    time.sleep(2)\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# initialize the driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# navigate to the website\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "# search for sneakers\n",
    "Product_select = driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "Product_select.send_keys('Sneakers')\n",
    "\n",
    "search= driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "\n",
    "# scrape data from multiple pages\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "\n",
    "start = 0\n",
    "end = 2 \n",
    "\n",
    "for page in range(start,end):\n",
    "    # wait for the page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # scrape brand names\n",
    "        Brand_tags = driver.find_elements(By.XPATH,'//div[@class=\"_2B099V\"]')\n",
    "        for b in Brand_tags:\n",
    "            brand_name = b.text\n",
    "            Brand.append(brand_name)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Brand not found\")\n",
    "        \n",
    "# scrape product descriptions\n",
    "Description_tags= driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for p in Description_tags:\n",
    "    Description = p.text\n",
    "    Product_Description.append(Description)\n",
    "\n",
    "# scrape prices\n",
    "try:\n",
    "Price_tags = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for p in Price_tags:\n",
    "    Product_Price = p.text\n",
    "    Price.append(Product_Price)\n",
    "    \n",
    "except NoSuchElementException:\n",
    "    print(\"Brand not found\")\n",
    "    break\n",
    "    \n",
    "# go to the next page\n",
    "next_page = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"][last()]')\n",
    "next_page.click()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d8697d3",
   "metadata": {},
   "source": [
    "print(len(Brand),len(Product_Description ),len(Price_tags))\n",
    "\n",
    "df= pd.DataFrame({'brand':Brand,'description':Product_Description,'price':Product_Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac638c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then \n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9da0cc4",
   "metadata": {},
   "source": [
    "#Lets first connect to the driver : \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "Product_select = driver.find_element(By.XPATH,'//input[@id=\"twotabsearchtextbox\"]')\n",
    "Product_select.send_keys('Laptop')          \n",
    "\n",
    "search = driver.find_element(By.XPATH,'//input[@id=\"nav-search-submit-button\"]')\n",
    "search.click()\n",
    "\n",
    "CPU_filter = driver.find_element(By.XPATH,'//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/span')\n",
    "CPU_filter.click()\n",
    "\n",
    "Title_Name = []\n",
    "Ratings = []\n",
    "Price = []\n",
    "\n",
    "# scrapping the names of the company from the page\n",
    "Title_tags = driver.find_elements(By.XPATH,'//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]')\n",
    "for t in Title_tags[0:10]:\n",
    "    Title=t.text\n",
    "    Title_Name.append(Title)\n",
    "    \n",
    "try: \n",
    "    # find all the rating tags and extract the text for the first 10 laptops\n",
    "    Ratings_Tags = driver.find_elements(By.XPATH,'//span[@class=\"a-icon-alt\"]')\n",
    "    for r in Ratings_Tags[0:10]:\n",
    "        rating = r.text\n",
    "except:\n",
    "    rating = 'N/A'\n",
    "    Ratings.append(rating)\n",
    "    \n",
    "#scrapping the Price \n",
    "try:\n",
    "    Price_Tags = driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "    for p in Price_Tags[0:10]:\n",
    "        Prices=p.text\n",
    "except:\n",
    "    price = 'N/A'\n",
    "    Price.append(Prices)\n",
    "\n",
    "print(len('Title_Name'),len('Ratings'),len('Price'))    \n",
    "\n",
    "# create a pandas DataFrame to store the scraped data\n",
    "df = pd.DataFrame({'Title':Title_Name,'Ratings':Ratings,'Price': Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79af7db3",
   "metadata": {},
   "source": [
    "Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1344f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect to the driver : \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")\n",
    "\n",
    "driver.get(\"https://www.azquotes.com/\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3463f795",
   "metadata": {},
   "source": [
    "Qoute = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "Qoute.click()\n",
    "\n",
    "qoute_titles = []\n",
    "Author_name = []\n",
    "Type_Qoute = []\n",
    "\n",
    "# Define variables for page range and job titles\n",
    "start = 0 \n",
    "end = 10 \n",
    "\n",
    "# Loop through the specified number of pages\n",
    "for page in range(start, end):\n",
    "    # Find all titles and authors on the current page\n",
    "    titles = driver.find_elements(By.XPATH, '//div[@class=\"title\"]')\n",
    "    authors = driver.find_elements(By.XPATH, '//div[@class=\"author\"]')\n",
    "    qoutes = driver.find_elements(By.XPATH, '//div[@class=\"tags\"]')\n",
    "    \n",
    "    # to create the loop \n",
    "    for title in titles:\n",
    "        qoute_titles.append(title.text)\n",
    "    for author in authors:\n",
    "        Author_name.append(author.text)\n",
    "    for qoute in qoutes:\n",
    "        Type_Qoute.append(qoute.text)\n",
    "                                  \n",
    "    \n",
    "    # Click the \"Next\" button to move to the next page\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,'//li[@class=\"next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        print(\"End of pages\")\n",
    "        break\n",
    "        \n",
    "time.sleep(5) \n",
    "    \n",
    "df = pd.DataFrame({'Title': qoute_titles, 'Author': Author_name, 'Type': Type_Qoute})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f20c71b",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/. This task will be done in following steps:\n",
    "\n",
    "First get the webpagehttps://www.jagranjosh.com/\n",
    "Then You have to click on the GK option\n",
    "Then click on the List of all Prime Ministers of India\n",
    "Then scrap the mentioned data and make theDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f648b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")\n",
    "driver.get(\"https://www.jagranjosh.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6281cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_button = driver.find_element(By.XPATH,'/html/body/div/div[1]/div/div[1]/div/div[5]/div/div[1]/header/div[3]/ul/li[9]')\n",
    "gk_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90d4bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# initialize the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Country Head\\Downloads\\chromedriver_win32.exe\")\n",
    "\n",
    "# navigate to the website\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n",
    "\n",
    "# click on the GK option\n",
    "gk_button = driver.find_element(By.XPATH,'/html/body/div/div[1]/div/div[1]/div/div[5]/div/div[1]/header/div[3]/ul/li[9]')\n",
    "gk_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "707f24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the List of all Prime Ministers of India link\n",
    "pm_link = driver.find_element(By.XPATH, '//a[text()=\"List of all Prime Ministers of India\"]')\n",
    "pm_link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29518519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Born-Dead, Term of office, Remarks]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# scrape the data\n",
    "Name_tags = driver.find_elements(By.XPATH,'//th[@style=\"width: 150px; height: 53px;\"]')\n",
    "Born_to_Dead_tags = driver.find_elements(By.XPATH,'//td[@style=\"width: 120px; text-align: center; vertical-align: middle;\"]')\n",
    "Term_of_office_tags = driver.find_elements(By.XPATH, '//td[@style=\"text-align: center; vertical-align: middle; width: 160px;\"]')\n",
    "Remarks_tags = driver.find_elements(By.XPATH, '//td[@style=\"width: 545px;\"]')\n",
    "\n",
    "Name = []\n",
    "Born_to_Dead = []\n",
    "Term_of_office = []\n",
    "Remarks = []\n",
    "\n",
    "for i in range(len(Name_tags)):\n",
    "    Name.append(Name_tags[i].text)\n",
    "    Born_to_Dead.append(Born_to_Dead_tags[i].text)\n",
    "    Term_of_office.append(Term_of_office_tags[i].text)\n",
    "    Remarks.append(Remarks_tags[i].text)\n",
    "\n",
    "# create the dataframe\n",
    "data = {'Name': Name, 'Born-Dead': Born_to_Dead, 'Term of office': Term_of_office, 'Remarks': Remarks}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# display the dataframe\n",
    "print(df)\n",
    "\n",
    "# close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ccdeb",
   "metadata": {},
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e. \n",
    "Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on leftside.\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa1b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
